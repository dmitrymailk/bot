{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/openai/whisper-large-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': '0d38672e0bbdbdc460af55b8bb84a15b2730db2819f2af64f9c777d4d586f2de', 'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00024414, 0.00048828,\n",
      "       0.0005188 ]), 'sampling_rate': 16000}\n",
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all, and can discover in it but little of rocky Ithaca. Linnell's pictures are a sort of Upguards and Adam paintings, and Mason's exquisite idylls are as national as a jingo poem. Mr. Burkett Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back before he says, like a shampooer in a Turkish bath, Next man!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=False,\n",
    "    use_safetensors=True,\n",
    "    use_flash_attention_2=True,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "print(sample)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    # max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=1,\n",
    "    # return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "result = pipe(dataset[0][\"audio\"])\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "data, samplerate = sf.read(\n",
    "    \"./experiments/temp_audio.wav\",\n",
    "    dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Шаблонный метод это поведенческий паттерн проектирования, который определяет скелет алгоритма, перекладывая ответственность за некоторые его шаги на подклассы. Паттерн позволяет подклассам переопределять шаги алгоритма, не меняя его общей структуры.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe({\"sampling_rate\": samplerate, \"raw\": data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voice input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gradio.app/guides/real-time-speech-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sampling_rate': 48000, 'raw': array([ 0.        ,  0.        ,  0.        , ...,  0.00072714,\n",
      "       -0.00054536, -0.00090893], dtype=float32)}\n",
      "{'sampling_rate': 48000, 'raw': array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "       -9.1555528e-05, -1.5259255e-04, -1.5259255e-04], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "def transcribe(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "    sample = {\"sampling_rate\": sr, \"raw\": y}\n",
    "    print(sample)\n",
    "    sf.write(\"./experiments/temp_audio.wav\", y, sr, subtype=\"PCM_24\")\n",
    "    result = pipe(sample)\n",
    "    result = result[\"text\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    transcribe,\n",
    "    gr.Audio(sources=[\"microphone\"]),\n",
    "    \"text\",\n",
    ")\n",
    "demo.launch(\n",
    "    inbrowser=True,\n",
    "    # server_port=7866,\n",
    ")\n",
    "clear_output()\n",
    "# print(\"Launched on http://127.0.0.1:7866/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/SYSTRAN/faster-whisper (НЕ РАБОТАЕТ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "rate = 48000\n",
    "samplerate = 16000\n",
    "data, samplerate = sf.read(\n",
    "    \"./experiments/temp_audio.wav\",\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "# Write out audio as 24bit PCM WAV\n",
    "# sf.write('stereo_file.wav', data, samplerate, subtype='PCM_24')\n",
    "data_16k = librosa.resample(\n",
    "    y=data,\n",
    "    orig_sr=rate,\n",
    "    target_sr=samplerate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe({\"sampling_rate\": samplerate, \"raw\": data_16k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = \"large-v3\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "# model = WhisperModel(\n",
    "#     model_size,\n",
    "#     device=\"cuda\",\n",
    "#     compute_type=\"float16\",\n",
    "# )\n",
    "\n",
    "# or run on GPU with INT8\n",
    "model = WhisperModel(\n",
    "    model_size,\n",
    "    device=\"cuda\",\n",
    "    compute_type=\"int8_float16\",\n",
    ")\n",
    "# or run on CPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language 'nn' with probability 0.563965\n",
      "[0.00s -> 8.24s]  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20.\n"
     ]
    }
   ],
   "source": [
    "segments, info = model.transcribe(\n",
    "    audio=data,\n",
    "    beam_size=5,\n",
    "    # language='ru'\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Detected language '%s' with probability %f\"\n",
    "    % (info.language, info.language_probability)\n",
    ")\n",
    "\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TranscriptionInfo(language='nn', language_probability=0.517578125, duration=9.72, duration_after_vad=9.72, all_language_probs=[('nn', 0.517578125), ('en', 0.081298828125), ('jw', 0.07403564453125), ('ko', 0.0728759765625), ('la', 0.04852294921875), ('ja', 0.0289764404296875), ('ru', 0.0257720947265625), ('haw', 0.0130615234375), ('es', 0.0113525390625), ('tr', 0.01108551025390625), ('fi', 0.01108551025390625), ('pl', 0.01058197021484375), ('fr', 0.0087738037109375), ('zh', 0.00798797607421875), ('pt', 0.00749969482421875), ('sv', 0.007442474365234375), ('de', 0.006671905517578125), ('ar', 0.005401611328125), ('it', 0.00492095947265625), ('th', 0.0048065185546875), ('uk', 0.004730224609375), ('ro', 0.0038604736328125), ('id', 0.0034618377685546875), ('nl', 0.0021839141845703125), ('km', 0.001941680908203125), ('si', 0.0018243789672851562), ('vi', 0.001781463623046875), ('da', 0.0016870498657226562), ('el', 0.0016870498657226562), ('cy', 0.00160980224609375), ('te', 0.0012683868408203125), ('cs', 0.00125885009765625), ('mi', 0.0011682510375976562), ('ta', 0.001071929931640625), ('ms', 0.0010309219360351562), ('ur', 0.0008449554443359375), ('no', 0.0008063316345214844), ('hu', 0.00074005126953125), ('sn', 0.0006842613220214844), ('ml', 0.0006632804870605469), ('tl', 0.0005922317504882812), ('hi', 0.00047588348388671875), ('fo', 0.0004558563232421875), ('az', 0.0003535747528076172), ('sk', 0.00034546852111816406), ('fa', 0.0003426074981689453), ('mn', 0.00030612945556640625), ('sa', 0.00027871131896972656), ('ca', 0.0002701282501220703), ('is', 0.00023281574249267578), ('br', 0.0002187490463256836), ('lv', 0.0001900196075439453), ('hr', 0.00018930435180664062), ('bn', 0.00015997886657714844), ('he', 0.0001538991928100586), ('eu', 0.000141143798828125), ('gl', 0.0001189112663269043), ('my', 0.0001049041748046875), ('hy', 9.191036224365234e-05), ('bg', 8.869171142578125e-05), ('bs', 7.355213165283203e-05), ('lt', 6.937980651855469e-05), ('et', 4.7326087951660156e-05), ('sr', 4.410743713378906e-05), ('sd', 4.374980926513672e-05), ('ne', 4.32133674621582e-05), ('yue', 4.291534423828125e-05), ('kk', 4.225969314575195e-05), ('be', 4.124641418457031e-05), ('lo', 4.094839096069336e-05), ('oc', 3.612041473388672e-05), ('yo', 3.528594970703125e-05), ('ht', 2.8371810913085938e-05), ('sw', 2.777576446533203e-05), ('yi', 1.9609928131103516e-05), ('bo', 1.6927719116210938e-05), ('mr', 1.6927719116210938e-05), ('ka', 1.5795230865478516e-05), ('sl', 1.1980533599853516e-05), ('af', 1.0251998901367188e-05), ('kn', 9.655952453613281e-06), ('pa', 9.5367431640625e-06), ('ps', 8.761882781982422e-06), ('as', 1.7285346984863281e-06), ('ln', 1.6689300537109375e-06), ('sq', 1.3709068298339844e-06), ('mt', 6.556510925292969e-07), ('am', 4.76837158203125e-07), ('mk', 4.76837158203125e-07), ('gu', 4.172325134277344e-07), ('su', 2.384185791015625e-07), ('tg', 1.7881393432617188e-07), ('so', 1.7881393432617188e-07), ('tt', 1.1920928955078125e-07), ('lb', 5.960464477539063e-08), ('tk', 5.960464477539063e-08), ('mg', 5.960464477539063e-08), ('ha', 5.960464477539063e-08), ('ba', 5.960464477539063e-08), ('uz', 0.0)], transcription_options=TranscriptionOptions(beam_size=5, best_of=5, patience=1, length_penalty=1, repetition_penalty=1, no_repeat_ngram_size=0, log_prob_threshold=-1.0, no_speech_threshold=0.6, compression_ratio_threshold=2.4, condition_on_previous_text=True, prompt_reset_on_temperature=0.5, temperatures=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0], initial_prompt=None, prefix=None, suppress_blank=True, suppress_tokens=[-1], without_timestamps=False, max_initial_timestamp=1.0, word_timestamps=False, prepend_punctuations='\"\\'“¿([{-', append_punctuations='\"\\'.。,，!！?？:：”)]}、'), vad_options=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/m-bain/whisperX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user-name-goes-here/.local/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "/home/user-name-goes-here/.local/lib/python3.10/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../home/user-name-goes-here/.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.1.2+cu121. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import gc\n",
    "\n",
    "device = \"cuda\"\n",
    "batch_size = 1  # reduce if low on GPU mem\n",
    "# compute_type = \"float16\"  # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "compute_type = \"int8\"  # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "\n",
    "# 1. Transcribe with original whisper (batched)\n",
    "model = whisperx.load_model(\n",
    "    \"large-v3\", device, compute_type=compute_type, vad_model=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ..., -0.0428772 ,\n",
       "       -0.04476929, -0.04360962], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file = \"sample_audio/temp_audio_ru.wav\"\n",
    "audio = whisperx.load_audio(audio_file)\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '0d38672e0bbdbdc460af55b8bb84a15b2730db2819f2af64f9c777d4d586f2de',\n",
       " 'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00024414, 0.00048828,\n",
       "        0.0005188 ]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "\t\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\"\n",
    ")\n",
    "sample = [\"audio\"]\n",
    "sampledataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00024414, 0.00048828,\n",
       "       0.0005188 ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(sample['array'], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: audio is shorter than 30s, language detection may be inaccurate.\n",
      "Detected language: ru (1.00) in first 30s of audio...\n",
      " Шаблонный метод – это поведенческий паттерн проектирования, который определяет скелет алгоритма, перекладывая ответственность за некоторые его шаги на подклассы. Паттерн позволяет подклассам переопределять шаги алгоритма, не меняя его общей структуры.\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(audio, batch_size=2)\n",
    "print(result[\"segments\"][0][\"text\"])  # before alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en (1.00) in first 30s of audio...\n",
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(np.array(sample['array'], dtype=np.float32), batch_size=2)\n",
    "print(result[\"segments\"][0][\"text\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sampling_rate': 48000, 'raw': array([0.        , 0.        , 0.        , ..., 0.02306926, 0.03407959,\n",
      "       0.04320243], dtype=float32)}\n",
      "Warning: audio is shorter than 30s, language detection may be inaccurate.\n",
      "Detected language: ru (1.00) in first 30s of audio...\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "samplerate = 16_000\n",
    "\n",
    "\n",
    "def transcribe(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "    sample = {\"sampling_rate\": sr, \"raw\": y}\n",
    "    print(sample)\n",
    "    data_16k = librosa.resample(\n",
    "        y=y,\n",
    "        orig_sr=sr,\n",
    "        target_sr=samplerate,\n",
    "    )\n",
    "    result = model.transcribe(data_16k, batch_size=batch_size)\n",
    "    if len(result[\"segments\"]) > 0:\n",
    "        result = result[\"segments\"][0][\"text\"]\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    transcribe,\n",
    "    gr.Audio(sources=[\"microphone\"]),\n",
    "    \"text\",\n",
    ")\n",
    "demo.launch(\n",
    "    inbrowser=True,\n",
    "    # server_port=7866,\n",
    ")\n",
    "clear_output()\n",
    "# print(\"Launched on http://127.0.0.1:7866/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
