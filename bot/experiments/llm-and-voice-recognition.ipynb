{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 -m vllm.entrypoints.api_server --model TheBloke/openchat-3.5-1210-AWQ --quantization awq --dtype auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Привет! К сожалению, я не могу заказывать пиццу напрямую, но я могу помочь тебе выбрать и заказать пиццу через доступные в регионе сервисы доставки. Какую пиццу ты хочешь?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sampling_rate': 48000, 'raw': array([0.        , 0.        , 0.        , ..., 0.00042726, 0.00030519,\n",
      "       0.00256355], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def chat(user_text=None):\n",
    "    initial_prompt = (\n",
    "        f\"GPT4 Correct User: Ты бот-помощник. Отвечай коротко и по существу.\\n\\n{user_text}<|end_of_turn|>GPT4 Correct Assistant:\\n\"\n",
    "    )\n",
    "    result = requests.post(\n",
    "        \"http://localhost:8000/generate\",\n",
    "        json={\n",
    "            \"prompt\": initial_prompt,\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 1024,\n",
    "        },\n",
    "    ).json()[\"text\"][0]\n",
    "    result = result[len(initial_prompt) :]\n",
    "    return result\n",
    "\n",
    "\n",
    "# chat(user_text=\"Привет.\")\n",
    "print(chat(user_text=\"Привет, я бы хотел, чтобы ты заказал мне пиццу.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voice recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': '0d38672e0bbdbdc460af55b8bb84a15b2730db2819f2af64f9c777d4d586f2de', 'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00024414, 0.00048828,\n",
      "       0.0005188 ]), 'sampling_rate': 16000}\n",
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all, and can discover in it but little of rocky Ithaca. Linnell's pictures are a sort of Upguards and Adam paintings, and Mason's exquisite idylls are as national as a jingo poem. Mr. Burkett Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back before he says, like a shampooer in a Turkish bath, Next man!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=False,\n",
    "    use_safetensors=True,\n",
    "    use_flash_attention_2=True,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=1,\n",
    "    return_timestamps=False,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "print(sample)\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/user-name-goes-here/.local/lib/python3.10/site-packages/gradio/queueing.py\", line 489, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/user-name-goes-here/.local/lib/python3.10/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/user-name-goes-here/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/user-name-goes-here/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1179, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/user-name-goes-here/.local/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/user-name-goes-here/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/user-name-goes-here/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/user-name-goes-here/.local/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1196/370329726.py\", line 7, in transcribe\n",
      "    sr, y = audio\n",
      "TypeError: cannot unpack non-iterable NoneType object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sampling_rate': 48000, 'raw': array([0.        , 0.        , 0.        , ..., 0.02005504, 0.01696385,\n",
      "       0.02246767], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def transcribe(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "    sample = {\"sampling_rate\": sr, \"raw\": y}\n",
    "    print(sample)\n",
    "    result = pipe(sample)\n",
    "    speech_text = result[\"text\"]\n",
    "    llm_response = chat(user_text=speech_text)\n",
    "    return f\"{speech_text}-----{llm_response}\"\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    transcribe,\n",
    "    gr.Audio(sources=[\"microphone\"]),\n",
    "    \"text\",\n",
    ")\n",
    "demo.launch(\n",
    "    inbrowser=True,\n",
    "    # server_port=7866,\n",
    ")\n",
    "clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
